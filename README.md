üõí ETL Pipeline - Lidl Data Project (Team A)
Este repositorio contiene un flujo de trabajo ETL (Extract, Transform, Load) automatizado, desarrollado en Python utilizando PySpark. El script est√° optimizado espec√≠ficamente para ejecutarse en entornos Windows, manejando la ingesta, limpieza y consolidaci√≥n de datos de ventas y clientes.

üöÄ Resumen del Workflow
El archivo principal etl_lidl_team_a.py orquesta las siguientes etapas:

üõ†Ô∏è Configuraci√≥n del Entorno: Ajuste autom√°tico de variables de entorno (Java/Hadoop) y mitigaci√≥n de errores de sockets en Windows.

üì• Extracci√≥n: Clonaci√≥n autom√°tica del repositorio de datos o uso de respaldo local.

ü•â Capa Bronze (Raw): Ingesta de m√∫ltiples formatos (.csv, .txt, .sql) y conversi√≥n a Parquet.

ü•à Capa Silver (Curated): Unificaci√≥n de datasets, normalizaci√≥n de strings, casteo de fechas y manejo de nulos.

üì§ Carga: Escritura del dataset maestro consolidado.

üìñ Documentaci√≥n T√©cnica Detallada
1. Configuraci√≥n del Entorno (Windows Optimization)
El script prepara el entorno de ejecuci√≥n para evitar conflictos comunes en Windows:

Configura JAVA_HOME (Java 17) y HADOOP_HOME din√°micamente.

Fix Cr√≠tico: Establece PYSPARK_PYTHON_WORKER_REUSE=0 para prevenir el error WinError 10038 (com√∫n en Python 3.12+ con Spark).

Inicializa una SparkSession local.

2. Adquisici√≥n de Datos
Verifica la existencia del directorio lidl_project_source.

Si no existe, ejecuta un git clone del repositorio fuente.

Fallback: Si falla la red, utiliza los datos locales.

3. Capa Bronze: Ingesta y Normalizaci√≥n de Formatos
Procesamiento de archivos crudos hacia formato Parquet (bronze/ventas/):
Archivo Fuente,Formato,Estrategia de Procesamiento
clientes_info.csv,CSV,Lectura est√°ndar con inferencia de headers.
clientes_extra.txt,TXT,Lectura como CSV sin header + Aplicaci√≥n de esquema manual (StructType).
clientes.sql,SQL,Parsing Avanzado: Extracci√≥n de valores INSERT INTO mediante Regex.  Workaround: Escritura intermedia a CSV temporal para evitar conflictos de memoria JVM/Python en Windows.

4. Capa Silver: Limpieza y Transformaci√≥nGeneraci√≥n del dataset maestro en silver/ventas/clientes_consolidado:Unificaci√≥n (Joins): Cruce de los tres dataframes usando codigo_cliente como llave primaria.Normalizaci√≥n de Texto:Trim: Eliminaci√≥n de espacios en blanco.InitCap: Formato de t√≠tulo para Nombres, Apellidos y Comunas.Lower: Estandarizaci√≥n a min√∫sculas para metadatos (Religi√≥n, Canales).Casteo de Tipos: Conversi√≥n de strings a objetos Date (formato yyyy-MM-dd).Manejo de Nulos:Textos $\rightarrow$ "sin_dato"Num√©ricos $\rightarrow$ 0üìã Gesti√≥n del ProyectoEl desarrollo y seguimiento de tareas de este ETL se gestion√≥ mediante un tablero Kanban en Azure DevOps.üíª Requisitos de Ejecuci√≥nPython 3.10+Java 17 (JDK)Binarios de Hadoop (winutils)Librer√≠as: pysparkDesarrollado por Team APor qu√© esta estructura funciona mejor:Jerarqu√≠a Visual: Uso de encabezados (#, ##) para separar claramente las secciones.Uso de Iconos: Los emojis (üõ†Ô∏è, üì•, ü•â) ayudan a identificar r√°pidamente las etapas del proceso sin tener que leer todo el texto.Tabla para la Capa Bronze: La informaci√≥n sobre los tipos de archivos (csv, txt, sql) se lee mucho mejor en una tabla que en una lista de texto plano.Destacados T√©cnicos: Se hace √©nfasis en el "Workaround de Windows" y el "Parsing de SQL", lo cual demuestra que el c√≥digo es robusto y resuelve problemas complejos.Diagramas Integrados: Las im√°genes est√°n colocadas estrat√©gicamente: el diagrama de flujo al principio para entender la l√≥gica, y el Kanban al final para mostrar la metodolog√≠a de trabajo.
