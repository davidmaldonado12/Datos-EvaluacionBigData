# üõí Pipeline ETL - Proyecto Lidl (Equipo A)

Este repositorio contiene la implementaci√≥n de un proceso **ETL (Extracci√≥n, Transformaci√≥n y Carga)** automatizado en Python utilizando **PySpark**. El script principal, `etl_lidl_team_a.py`, est√° optimizado espec√≠ficamente para entornos **Windows**, manejando la ingesta, limpieza y consolidaci√≥n de datos de clientes.

---

## üìä Diagrama Workflow y Arquitectura

Los siguientes esquemas ilustran la arquitectura del pipeline, desde la configuraci√≥n del entorno hasta la carga final en la capa Silver.

<div align="center">
  <img src="https://github.com/user-attachments/assets/95fff1af-71a7-4a2c-8675-2987ca0b5550" alt="Workflow Diagram" width="90%">
</div>

<div align="center">
  <img width="3091" height="1763" alt="Untitled diagram-2025-11-28-233736" src="https://github.com/user-attachments/assets/787d5abe-02d1-4613-b34a-0eefe0fd454b" />
</div>

---

## üõ†Ô∏è Descripci√≥n del Workflow

A continuaci√≥n se detalla el funcionamiento t√©cnico de cada etapa del script:

### 1. Configuraci√≥n del Entorno (Windows Optimization)
El script prepara autom√°ticamente el entorno de ejecuci√≥n para mitigar errores comunes de Hadoop y Spark en Windows:
* **Variables de Entorno:** Configura din√°micamente `JAVA_HOME` (Java 17) y `HADOOP_HOME`.
* **Fix de Sockets (WinError 10038):** Establece `PYSPARK_PYTHON_WORKER_REUSE=0`, crucial para evitar fallos de conexi√≥n en versiones recientes de Python.
* **Inicializaci√≥n:** Crea una `SparkSession` local bajo el nombre "Lidl_ETL_Team_A".

### 2. Extracci√≥n de Datos
El sistema asegura la disponibilidad de los datos fuente mediante una l√≥gica de redundancia:
1.  Verifica la existencia del directorio `lidl_project_source`.
2.  Si no existe, intenta clonar el repositorio oficial desde **GitHub**.
3.  **Fallback:** Si la clonaci√≥n falla, utiliza los archivos locales como respaldo.

### 3. Capa Bronze (Ingesta Raw)
Se procesan archivos de diversos formatos y se estandarizan a **Parquet** en `bronze/ventas/`.

| Archivo Fuente | Formato | Estrategia de Procesamiento |
| :--- | :--- | :--- |
| **`clientes_info.csv`** | CSV | Lectura est√°ndar con inferencia de cabeceras. |
| **`clientes_extra.txt`** | TXT | Lectura sin header + Aplicaci√≥n de esquema manual (`StructType`). |
| **`clientes.sql`** | SQL | **Parsing Avanzado:** Extracci√≥n de valores `INSERT` v√≠a Regex.<br>**Workaround:** Escritura intermedia a CSV temporal para evitar conflictos de memoria JVM en Windows. |

### 4. Capa Silver (Transformaci√≥n y Limpieza)
Generaci√≥n del dataset maestro consolidado mediante las siguientes reglas de negocio:

* üîó **Unificaci√≥n:** Join de los tres datasets usando `codigo_cliente` como llave primaria.
* üìù **Normalizaci√≥n de Texto:**
    * `Trim`: Eliminaci√≥n de espacios excedentes.
    * `InitCap`: Formato de T√≠tulo para *Nombres*, *Apellidos* y *Comunas*.
    * `Lower`: Min√∫sculas para *Religi√≥n*, *Alimentaci√≥n* y *Canales*.
* üìÖ **Estandarizaci√≥n Temporal:** Conversi√≥n de strings a objetos `Date` (formato `yyyy-MM-dd`).
* üö´ **Manejo de Nulos:**
    * Campos de texto $\rightarrow$ `"sin_dato"`
    * Campos num√©ricos $\rightarrow$ `0`

### 5. Carga Final
El resultado limpio y unificado se escribe en formato **Parquet** en la ruta:
> `silver/ventas/clientes_consolidado`

---

## üìã Gesti√≥n del Proyecto

El seguimiento de tareas y evolutivos del desarrollo se realiz√≥ mediante un tablero Kanban en Azure DevOps.

<div align="center">
  <img src="https://github.com/user-attachments/assets/32e7144f-22e9-402e-9e71-93e0318d8ed2" alt="Azure Kanban Board" width="60%">
</div>
